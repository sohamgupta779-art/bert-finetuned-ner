{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resume NER — Step 1\n",
        "Environment setup and dataset check\n"
      ],
      "metadata": {
        "id": "U1kVtyYA1h5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets seqeval evaluate accelerate gradio\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P812lkzt1jlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, sys\n",
        "import transformers, datasets\n",
        "\n",
        "print(\"Python:\", sys.version.splitlines()[0])\n",
        "print(\"PyTorch:\", torch.__version__, \"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Datasets:\", datasets.__version__)\n"
      ],
      "metadata": {
        "id": "JWIIm2JH1vjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets==2.19.1\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rgAPVNfa2xDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "d84da4f3"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "try:\n",
        "    dataset = load_dataset(\"conll2003\", revision=\"refs/convert/parquet\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load dataset as parquet: {e}\")\n",
        "\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_CHECKPOINT = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GsMXoXRQ368c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(\"Number of labels:\", num_labels)\n",
        "print(\"Labels:\", label_list)\n",
        "\n",
        "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "id_to_label = {i: l for i, l in enumerate(label_list)}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r5UT3zsD3_YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    all_labels = []\n",
        "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(labels[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        all_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = all_labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "5cuisOTY4I_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_datasets\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cr9XAQJM4L3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id_to_label,\n",
        "    label2id=label_to_id\n",
        ")\n"
      ],
      "metadata": {
        "id": "9DkYxTTa4i-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    true_labels = [\n",
        "        [id_to_label[l] for l in label if l != -100]\n",
        "        for label in labels\n",
        "    ]\n",
        "    true_predictions = [\n",
        "        [id_to_label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "nrgU_riR4q6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "5pOvQm3N4s0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"bert-finetuned-ner\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "HwdF_dBi4ztt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "6MvzawUu5XAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "f0G-Tiaw5aXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PSRcBem36BfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"bert-ner-model\")\n",
        "tokenizer.save_pretrained(\"bert-ner-model\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oUkJ7EPP8Ov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "O56925tX8eg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"bert-finetuned-ner\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JMvvFL7488jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=\"bert-ner-model\",\n",
        "    tokenizer=\"bert-ner-model\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "def ner_inference(text):\n",
        "    results = ner_pipeline(text)\n",
        "    return results\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ner_inference,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Paste resume text here...\"),\n",
        "    outputs=\"json\",\n",
        "    title=\"Resume NER Demo\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4ISoSOmM9UtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf bert-finetuned-ner && git clone https://huggingface.co/spaces/soh7/bert-finetuned-ner\n",
        "%cd bert-finetuned-ner\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LPyAc7o5-DhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load fine-tuned model from Hugging Face Hub\n",
        "# Make sure you push your model with trainer.push_to_hub() first\n",
        "ner_pipeline = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=\"soh7/bert-finetuned-ner\",   # change if your model repo is named differently\n",
        "    tokenizer=\"soh7/bert-finetuned-ner\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "def ner_inference(text):\n",
        "    return ner_pipeline(text)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ner_inference,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Paste resume text here...\"),\n",
        "    outputs=\"json\",\n",
        "    title=\"Resume NER Demo\"\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "D0Dr4nZNGt57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "transformers\n",
        "torch\n",
        "gradio\n"
      ],
      "metadata": {
        "id": "KSX7dXj2G5jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "CkhGLz0J-WKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "# Resume Named Entity Recognition (NER) with Hugging Face\n",
        "\n",
        "## 📌 Overview\n",
        "This project fine-tunes **BERT (bert-base-cased)** on the CoNLL-2003 dataset for **Named Entity Recognition (NER)**.\n",
        "The same pipeline can be applied to **resume parsing** (extracting skills, degrees, companies, job titles).\n",
        "\n",
        "## 🚀 Features\n",
        "- Fine-tuned Transformer (BERT) for token classification\n",
        "- Achieves high F1 on validation set\n",
        "- Interactive Gradio demo to test NER on custom text\n",
        "- Ready for deployment to Hugging Face Spaces\n",
        "\n",
        "## 🛠️ Tech Stack\n",
        "- Python, PyTorch\n",
        "- Hugging Face Transformers & Datasets\n",
        "- Gradio (demo UI)\n",
        "- Google Colab (training)\n",
        "\n",
        "## 📊 Example\n",
        "**Input**\n",
        "\n",
        "```\n",
        "John Doe is a Software Engineer at Google, who graduated from Stanford University.\n",
        "```\n",
        "\n",
        "**Output**\n",
        "```json\n",
        "[\n",
        "  {\"entity\": \"PER\", \"word\": \"John Doe\"},\n",
        "  {\"entity\": \"ORG\", \"word\": \"Google\"},\n",
        "  {\"entity\": \"JOB\", \"word\": \"Software Engineer\"},\n",
        "  {\"entity\": \"LOC\", \"word\": \"Stanford University\"}\n",
        "]\n",
        "```\n",
        "Model:https://huggingface.co/soh7/bert-finetuned-ner\n",
        "\n",
        "Demo: https://huggingface.co/spaces/soh7/bert-finetuned-ner\n",
        "\n",
        "Code: https://github.com/sohamgupta779-art/bert-finetuned-ner.git\n"
      ],
      "metadata": {
        "id": "4YecFtCKBemb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat README.md\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rAZVnyiRBrQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVNEa1CuyCd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://$GITHUB_USER:$GITHUB_TOKEN@github.com/sohamgupta779-art/bert-finetuned-ner.git\n",
        "%cd bert-finetuned-ner\n"
      ],
      "metadata": {
        "id": "us52o-ZhyOkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "# Resume Named Entity Recognition (NER) with Hugging Face\n",
        "\n",
        "## 📌 Overview\n",
        "This project fine-tunes **BERT (bert-base-cased)** on the CoNLL-2003 dataset for **Named Entity Recognition (NER)**.\n",
        "The same pipeline can be applied to **resume parsing** (extracting skills, degrees, companies, job titles).\n",
        "\n",
        "## 🚀 Features\n",
        "- Fine-tuned Transformer (BERT) for token classification\n",
        "- Interactive Gradio demo\n",
        "- Ready for Hugging Face Spaces\n",
        "\n",
        "## 🛠️ Tech Stack\n",
        "- Python, PyTorch\n",
        "- Hugging Face Transformers\n",
        "- Gradio\n",
        "- Google Colab\n",
        "\n",
        "## 📊 Example\n",
        "**Input**\n",
        "John Doe worked at Google as a Software Engineer after studying at Stanford University.\n",
        "\n",
        "**Output**\n",
        "```json\n",
        "[\n",
        "  {\"entity\": \"PER\", \"word\": \"John Doe\"},\n",
        "  {\"entity\": \"ORG\", \"word\": \"Google\"},\n",
        "  {\"entity\": \"JOB\", \"word\": \"Software Engineer\"},\n",
        "  {\"entity\": \"LOC\", \"word\": \"Stanford University\"}\n",
        "]\n",
        "Model:https://huggingface.co/soh7/bert-finetuned-ner\n",
        "\n",
        "Demo: https://huggingface.co/spaces/soh7/bert-finetuned-ner\n",
        "\n",
        "Code: https://github.com/sohamgupta779-art/bert-finetuned-ner.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nD_mGoWykUU",
        "outputId": "53a1c77f-886c-492e-a944-788b20244ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "transformers\n",
        "torch\n",
        "gradio\n",
        "\n"
      ],
      "metadata": {
        "id": "IgjCk9LRy_SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YC_b8-oGzcSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Q8y4CZurzOmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r2QGiqOTzZth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "rtEl2ZCc05fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XiWNFQRV0UQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}